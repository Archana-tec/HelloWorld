'''
The following code is intended to deploy, run, and schedule UC3's training and inferencing pipeline.
'''


import os
import datetime
from azureml.core import Workspace, Experiment, Environment, Dataset
from azureml.core.compute import AmlCompute
from azureml.core.datastore import Datastore
from azureml.core.runconfig import RunConfiguration, DockerConfiguration
from azureml.pipeline.core import Pipeline, PipelineData, Schedule, ScheduleRecurrence, PublishedPipeline
from azureml.pipeline.steps import PythonScriptStep
from azureml.data import OutputFileDatasetConfig
import logging
from azureml.core import Dataset, Datastore
from os import environ
from azureml.core.authentication import ServicePrincipalAuthentication
import datetime

# Set up logging configuration
logging.basicConfig(level=logging.INFO, format='%(asctime)s :: [%(levelname)s] :: %(message)s')

# Print current working directory to ensure that pipeline is using this folder as its working directory
# Assert that cwd is pointing to the src folder
'''
cwd = os.getcwd()
cwd1=cwd+'\\ADA_ADO\\mlops\\uc3\\src'
logging.info("-------- CWD: {} ------- ".format(cwd1))
assert cwd1.split("\\")[-1] == "src", "Current working directory is {}".format(cwd1.split("\\")[-1])
'''
# ---------- Environment Variables ---------- #
env = os.environ["ENV"]

# ---------- Workspace Variables ---------- #
workspace_name = os.environ["WORKSPACENAME"]
resource_group = os.environ["RESOURCEGROUP"]
subscription_id = os.environ["SUBSCRIPTION"]

tenant_id = os.environ["SPTENANTID"]
principal_id = os.environ["SPPRINCIPALID"]
client_secret = os.environ["SPCLIENTSECRET"]

# Authenticating into the ml workspace


svc_pr = ServicePrincipalAuthentication(
       tenant_id=tenant_id,
       service_principal_id=principal_id,
       service_principal_password=client_secret,
       _enable_caching=True)
print(svc_pr)


# ---------- Compute Variables ---------- #
compute_name = os.environ["COMPUTENAME"]

# ---------- Docker Environment Variables ---------- #
environment_name = os.environ["ENVIRONMENTNAME"]

# ---------- Pipeline Variables ---------- #
forecast_date = 'default'

# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<AUTO GENERATED BY CONFLICT EXTENSION<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< dev_ada_ae_rtio
# pipeline_name = "UC3_Inference_Pipeline_dev"
# ====================================AUTO GENERATED BY CONFLICT EXTENSION====================================

if env=='dev':
    pipeline_name = "UC3_Inference_Pipeline_dev" 
elif env=='uat':
    pipeline_name = "UC3_Inference_Pipeline_UAT"
else:
    pipeline_name = "UC3_Inference_Pipeline"
# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>AUTO GENERATED BY CONFLICT EXTENSION>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> feature/ada_ae_rtio_dev_300623

pipeline_schedule_name = "uc3_inferencing_pipeline_schedule"


# Output variables being used
logging.info("---------- Environment Variables ----------")
logging.info("Environment: {}".format(env))

logging.info("---------- Workspace Variables ----------")
logging.info("Workspace Name: {}".format(workspace_name))
logging.info("Resource Group: {}".format(resource_group))
logging.info("Subscription Id: {}".format(subscription_id))

logging.info("---------- Compute Variables ----------")
logging.info("Compute Name: {}".format(compute_name))

logging.info("---------- Docker Environment Variables ----------")
logging.info("Environment Name: {}".format(environment_name))

logging.info("---------- Pipeline Variables ----------")

logging.info("Pipeline Name: {}".format(pipeline_name))
logging.info("Pipeline Schedule Name: {}".format(pipeline_schedule_name))

# Authenticating into the ml workspace
ws = Workspace.get(
    name=workspace_name,
    subscription_id=subscription_id,
    resource_group=resource_group,
    auth = svc_pr
)

# ---------- Disable Old Experiment and Schedule ---------- #
def disable_pipeline_and_schedule(ws, scheduler_name, pipeline_name):
    logging.info(f"Disableing Pipeline and Schedule")
    logging.info(f"- Scheduler name = {scheduler_name}")
    logging.info(f"- Pipeline name = {pipeline_name}")
    # Find and disable scoring schedules
    pipeline_schedules = Schedule.list(
        workspace=ws
    )

    # Find and disable azure ml pipelines
    published_pipelines = PublishedPipeline.list(
        workspace=ws
    )

    schedules = [
        schedule for schedule in pipeline_schedules
        if schedule.name in list([scheduler_name])
    ]
    logging.info(f"{len(schedules)} number of schedules found")

    # Disable old schedule
    for schedule in schedules:
        schedule.disable()
        logging.info("{} succcessfully disabled".format(schedule._name))
    logging.info("Successfully disabled all pipeline schedules")

    pipelines = [
        pipeline for pipeline in published_pipelines
        if pipeline._name in list([pipeline_name])
    ]
    logging.info("{} number of pipelines found".format(len(pipelines)))

    # Disable old pipelines
    for pipeline in pipelines:
        PublishedPipeline.get(
            workspace = ws,
            id = pipeline.id
        ).disable()
        logging.info("{} succcessfully disabled".format(pipeline._name))
    logging.info("Successfully disabled all pipelines")


# Grab the workspace and landing prod/dev/uat datastore
logging.info("environment coming as {}".format(env))

if env=='dev':
    datastore_nam = 'ada_landed_data_storage_dev'
    def_datastore_name = 'ada_results_storage_dev'
    out_datastore_name='ada_output_storage'
    environment = 'DEV'
else:
    datastore_nam='ada_landed_data_storage_prod'
    def_datastore_name ='ada_results_storage_prod'
    out_datastore_name='ada_output_storage_prod'
    if env=='uat':
        environment = 'UAT'
    else:
        environment = 'PROD'
logging.info("environment is: {}".format(environment))
ada_prod = Datastore.get(
    workspace = ws,
    datastore_name = datastore_nam
)


logging.info("Retrieved landing datastore with name: {}".format(ada_prod.name))

def_blob_store = Datastore.get(
    workspace = ws,
    datastore_name = def_datastore_name
)
out_blob_store=Datastore.get(
    workspace = ws,
    datastore_name = out_datastore_name
)
logging.info("Retrieved workspace datastore with name: {}".format(def_blob_store.name))

# Grab the compute cluster that the pipeline will run on
ml_compute_instance = AmlCompute(
    workspace = ws,
    name = compute_name
)

# Setting up compute environment
my_env = Environment.get(
    workspace = ws,
    name = environment_name
)
run_config = RunConfiguration()
docker_config = DockerConfiguration(use_docker = True)
run_config.docker = docker_config
run_config.environment = my_env
logging.info("Set up compute with name = {} and environment with name = {}".format(compute_name, environment_name))

# Retrieve datasets from the "ada_landed_data_storage_prod" datastore and mount it to the pipeline
PCS_tags_dataset = Dataset.File.from_files(
    path = (def_blob_store, 'ADA_processed_datasets/PCS_tags/**'),
    validate=False
)
PCS_tags_input = PCS_tags_dataset.as_named_input('PCS_Tags').as_mount()
logging.info("Retrieved dataset with name = PCS_Tags and placed as mount on the compute instance of the pipeline")

PDS_dataset = Dataset.File.from_files(
    path = (ada_prod, 'kd-int075-pds-ada/**'),
    validate = False
)
PDS_input = PDS_dataset.as_named_input('PDS').as_mount()
logging.info("Retrieved dataset with name = PDS and placed as mount on the compute instance of the pipeline")

shutdowns_dataset = Dataset.File.from_files(
    path = (ada_prod, 'kd-int066-msf-shutdown-plans-ada/**'),
    validate = False
)
shutdowns_input = shutdowns_dataset.as_named_input('Shutdown_Schedule').as_mount()
logging.info("Retrieved dataset with name = Shutdown_Schedule and placed as mount on the compute instance of the pipeline")

UC3_manual_inputs_files = Dataset.File.from_files(
    path = (ada_prod,'manual-uploads/UC3_Manual_Inputs/**'),
    validate = False
)
UC3_manual_inputs = UC3_manual_inputs_files.as_named_input('UC3_Manual_Inputs').as_mount()
logging.info("Retrieved dataset with name = UC3_Manual_Inputs and placed as mount on the compute instance of the pipeline")

# Model output, cache, and raw data locations 
dataset_interim = PipelineData(
    name = 'UC3_Interim' 
)

belt_tonnage_interim = PipelineData(
    name = 'UC3_Belt_Tonnage'
)
belt_time_interim = PipelineData(
    name = 'UC3_Belt_Time' 
)

AF_tonnage_interim = PipelineData(
    name = 'UC3_Apron_Feeder_Tonnage'
)

AF_time_interim = PipelineData(
    name = 'UC3_Apron_Feeder_Time'
)

model_outputs_interim_belt = PipelineData(
    name = 'UC3_Model_Outputs_interim_belt' 
)

model_outputs_interim_AF= PipelineData(
    name = 'UC3_Model_Outputs_interim_AF' 
)
if environment=='UAT':
    output_cache = OutputFileDatasetConfig(
        destination = (out_blob_store, 'ADA_UC3_WRF/UAT')
    )
else:
    output_cache = OutputFileDatasetConfig(
        destination = (out_blob_store, 'ADA_UC3_WRF')
    )
# Defining the steps required for the Azure ML Pipeline
#source_directory = "..\\..\\..\\..\\"
source_directory = "./"
print(source_directory)
# Step 1: Data Prep
data_prep_step = PythonScriptStep(
    script_name = "ADA_UC3_WRF/uc3_inference_dataprep.py",
    source_directory = source_directory,
    inputs = [
        PCS_tags_input, 
        PDS_input, 
        shutdowns_input, 
        UC3_manual_inputs
    ],
    outputs = [
        dataset_interim
    ],
    arguments = [
        "--PCS_tags", PCS_tags_input,
        "--PDS", PDS_input,
        "--shutdowns", shutdowns_input,
        "--UC3_manual_inputs", UC3_manual_inputs,
        "--output_dir", dataset_interim
    ],
    compute_target = ml_compute_instance,
    runconfig = run_config,
    allow_reuse = False
)

# Step 2: Belt Tonnage Model 
belt_tonnage_model_step = PythonScriptStep(
    script_name = "ADA_UC3_WRF/uc3_inference_belt_tonnage_model.py",
    source_directory = source_directory,
    inputs=[
        dataset_interim,
        UC3_manual_inputs
    ],
    outputs = [
        belt_tonnage_interim
    ],
    arguments = [
        "--input_data", dataset_interim,
        "--manual_inputs", UC3_manual_inputs,
        "--forecast_date", forecast_date,
        "--output_dir", belt_tonnage_interim
    ],
    compute_target = ml_compute_instance,
    runconfig = run_config,
    allow_reuse = False
)

# Step 3: Belt Time Model
belt_time_model_step = PythonScriptStep(
    script_name = "ADA_UC3_WRF/uc3_inference_belt_time_model.py",
    source_directory = source_directory,
    inputs = [
        dataset_interim,
        UC3_manual_inputs
    ],
    outputs = [
        belt_time_interim
    ],
    arguments = [
        "--input_data", dataset_interim,
        "--manual_inputs", UC3_manual_inputs,
        "--forecast_date", forecast_date,
        "--output_dir", belt_time_interim
    ],
    compute_target = ml_compute_instance,
    runconfig = run_config,
    allow_reuse = False
)

# Step 4: AF Tonnage Model
AF_tonnage_model_step = PythonScriptStep(
    script_name = "ADA_UC3_WRF/uc3_inference_AF_tonnage_model.py",
    source_directory = source_directory,
    inputs = [
        dataset_interim,
        UC3_manual_inputs
    ],
    outputs=[
        AF_tonnage_interim
    ],
    arguments = [
        "--input_data", dataset_interim,
        "--manual_inputs", UC3_manual_inputs,
        "--forecast_date", forecast_date,
        "--output_dir", AF_tonnage_interim
    ],
    compute_target = ml_compute_instance,
    runconfig = run_config,
    allow_reuse = False
)

# Step 5: AF Time Model
AF_time_model_step = PythonScriptStep(
    script_name="ADA_UC3_WRF/uc3_inference_AF_time_model.py",
    source_directory = source_directory,
    inputs = [
        dataset_interim,
        UC3_manual_inputs
    ],
    outputs = [
        AF_time_interim
    ],
    arguments = [
        "--input_data", dataset_interim,
        "--manual_inputs", UC3_manual_inputs,
        "--forecast_date", forecast_date,
        "--output_dir", AF_time_interim
    ],
    compute_target = ml_compute_instance,
    runconfig = run_config,
    allow_reuse = False
)

# Step 6: Post Processing Belt
postprocessing_step_belt = PythonScriptStep(
    script_name = "ADA_UC3_WRF/uc3_inference_postprocessing_belt.py",
    inputs = [
        dataset_interim,
        UC3_manual_inputs,
        belt_tonnage_interim,
        belt_time_interim
    ],
    outputs = [
        model_outputs_interim_belt
    ],
    arguments = [
        "--dataprep_input", dataset_interim,
        "--manual_inputs", UC3_manual_inputs,
        "--belt_tonnage_input", belt_tonnage_interim,
        "--belt_time_input", belt_time_interim,
        "--forecast_date", forecast_date,
        "--output_dir", model_outputs_interim_belt
    ],
    compute_target = ml_compute_instance,
    source_directory = source_directory,
    runconfig = run_config,
    allow_reuse = False
)

# Step 7: Postprocessing AF
postprocessing_step_AF = PythonScriptStep(
    script_name = "ADA_UC3_WRF/uc3_inference_postprocessing_AF.py",
    inputs = [
        dataset_interim,
        UC3_manual_inputs,
        belt_tonnage_interim,
        belt_time_interim,
        AF_tonnage_interim,
        AF_time_interim
    ],
    outputs = [
        model_outputs_interim_AF
    ],
    arguments = [
        "--dataprep_input", dataset_interim,
        "--manual_inputs", UC3_manual_inputs,
        "--belt_tonnage_input", belt_tonnage_interim,
        "--belt_time_input", belt_time_interim,
        "--AF_tonnage_input", AF_tonnage_interim,
        "--AF_time_input", AF_time_interim,
        "--forecast_date", forecast_date,
        "--output_dir", model_outputs_interim_AF
    ],
    compute_target = ml_compute_instance,
    source_directory = source_directory,
    runconfig = run_config,
    allow_reuse = False
) 

# Step 8: Postprocessing combined
postprocessing_step_combined = PythonScriptStep(
    script_name = "ADA_UC3_WRF/uc3_inference_postprocessing_combined.py",
    inputs = [
        model_outputs_interim_belt,
        model_outputs_interim_AF
    ],
    outputs = [
        output_cache
    ],
    arguments = [
        "--forecast_date", forecast_date,
        "--belt_input", model_outputs_interim_belt,
        "--AF_input", model_outputs_interim_AF,
        "--output_dir", output_cache
    ],
    compute_target = ml_compute_instance,
    source_directory = source_directory,
    runconfig = run_config,
    allow_reuse = False
)

logging.info("Registered python script steps for Azure ML pipeline")

# Set up and run as experiment
pipeline = Pipeline(
    workspace = ws, 
    steps = [
        data_prep_step,
        belt_tonnage_model_step,
        belt_time_model_step,
        AF_tonnage_model_step,
        AF_time_model_step,
        postprocessing_step_belt,
        postprocessing_step_AF,
        postprocessing_step_combined
    ]
)

pipeline_run = Experiment(
    workspace = ws,
    name = pipeline_name
).submit(
    pipeline
)
logging.info("Submitting experiment with name: {}".format(pipeline_name))

pipeline_run.wait_for_completion(
    show_output = False
)
logging.info("Succesfully completed pipeline experiment")

#------------DIsable existing Schedule-----------------------#
disable_pipeline_and_schedule(ws, pipeline_schedule_name, pipeline_name)

# ---------- Publish New Pipeline and Create Schedule ---------- #
# Publish pipeline
published_pipeline = pipeline_run.publish_pipeline(
     name = pipeline_name,
     description = "Weekly pipeline for RioTinto ADA Use Case 3 Inferencing Pipeline",
     version= "1.0"
)
logging.info("Successfully published pipeline to Azure ML workspace")

pipeline_id = published_pipeline.id
experiment_name = pipeline_name
start_time = datetime.datetime(year = 2023, month = 5, day = 23, hour = 1)

# Set up a recurring schedule
if environment=='DEV':
    recurrence = ScheduleRecurrence(
        frequency = "Week", 
        interval = 1,
        start_time = start_time
    )
else:
    recurrence = ScheduleRecurrence(
        frequency = "Day", 
        interval = 1,
        start_time = start_time
    )

recurring_schedule = Schedule.create(
    workspace = ws,
    name = pipeline_schedule_name, 
    description="Use Case 3 Inferencing Pipeline Schedule",
    pipeline_id = pipeline_id, 
    experiment_name = experiment_name, 
    recurrence = recurrence
)
logging.info("Successfully created schedule for the published pipeline")
